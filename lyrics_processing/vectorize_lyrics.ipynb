{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load the .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Define the embedding model to use\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\"\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load lyrics from a root folder of text files.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The path to the root folder containing the text files.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list of Document\n",
    "        A list of documents, each containing the text from one of the text files.\n",
    "    \"\"\"\n",
    "\n",
    "    text_loader_kwargs = {\"encoding\": \"utf8\"}\n",
    "    loader = DirectoryLoader(\n",
    "        path=folder_path,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs=text_loader_kwargs,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    for doc in docs:\n",
    "        source_metadata = doc.metadata[\"source\"]\n",
    "\n",
    "        title = os.path.basename(source_metadata).replace(\".txt\", \"\")\n",
    "        file_folder = os.path.dirname(source_metadata)\n",
    "        artist = os.path.basename(file_folder)\n",
    "\n",
    "        doc.metadata[\"artist\"] = artist\n",
    "        doc.metadata[\"title\"] = title\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the lyrics from the root folder\n",
    "\n",
    "# Define the root folder containing the lyrics\n",
    "lyrics_path = \"lyrics\"\n",
    "\n",
    "# Load the lyrics from the root folder\n",
    "docs_lyrics = load_lyrics_from_folder(lyrics_path)\n",
    "print(f\"Number of documents with lyrics {len(docs_lyrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove duplicates based on page_content column and keep the first one based on title and artist length (shortest)\n",
    "\n",
    "# Create a DataFrame from the documents\n",
    "df_docs = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"artist\": doc.metadata[\"artist\"],\n",
    "            \"title\": doc.metadata[\"title\"],\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"source\": doc.metadata[\"source\"],\n",
    "        }\n",
    "        for doc in docs_lyrics\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Find all duplicates based on page_content column\n",
    "duplicates = df_docs[df_docs.duplicated(\"page_content\", keep=False)].copy()\n",
    "\n",
    "# Add title and artist length columns\n",
    "duplicates[\"title_len\"] = duplicates[\"title\"].apply(len)\n",
    "duplicates[\"artist_len\"] = duplicates[\"artist\"].apply(len)\n",
    "\n",
    "# Sort by title length and artist length in ascending order\n",
    "duplicates = duplicates.groupby(\"page_content\").apply(\n",
    "    lambda x: x.sort_values([\"title_len\", \"artist_len\"], ascending=[True, True]),\n",
    "    include_groups=False,\n",
    ")\n",
    "\n",
    "# Reset index and mark all duplicates except the first one\n",
    "duplicates = duplicates.reset_index()\n",
    "duplicates = duplicates[duplicates.duplicated(\"page_content\", keep=\"first\")]\n",
    "\n",
    "# Remove the marked duplicates from the file system\n",
    "for index, row in duplicates.iterrows():\n",
    "    source = row[\"source\"]\n",
    "    os.remove(source)\n",
    "    print(f\"Removed {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete non-english documents with non-ascii characters in the page_content column\n",
    "\n",
    "# Define the pattern to match non-ascii characters\n",
    "pattern = r\"[^\\u0000-\\u007F]\"\n",
    "\n",
    "# Find all documents with non-ascii characters in the page_content column and remove them\n",
    "for doc in df_docs.itertuples():\n",
    "    if re.search(pattern, doc.page_content):\n",
    "        os.remove(doc.source)\n",
    "        print(f\"Removed {doc.source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the documents into smaller chunks\n",
    "\n",
    "# Split the documents into smaller chunks of text with a maximum size of 300 characters and an overlap of 150 characters\n",
    "# The documents are split based on the separators \"\\n\\n\" and \"\\n\"\n",
    "# So firstly we split lyrics in respect to paragraphs and then split each paragraph in respect to lines\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\"], chunk_size=300, chunk_overlap=150\n",
    ")\n",
    "documents = text_splitter.split_documents(docs_lyrics)\n",
    "print(f\"Number of documents after splitting {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find duplicates in the split documents based on source and page_content columns and remove them from the documents list\n",
    "\n",
    "# Create a DataFrame from the documents\n",
    "df_docs = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"artist\": doc.metadata[\"artist\"],\n",
    "            \"title\": doc.metadata[\"title\"],\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"source\": doc.metadata[\"source\"],\n",
    "        }\n",
    "        for doc in documents\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Find all duplicates based on source and page_content columns leaving the first one\n",
    "duplicates = df_docs[df_docs.duplicated([\"source\", \"page_content\"])]\n",
    "print(\n",
    "    f\"Number of documents before removing duplicates {len(documents)} and number of duplicates {len(duplicates)}\"\n",
    ")\n",
    "\n",
    "# Remove the duplicates from the documents list by setting the duplicate elements to None\n",
    "for duplicate in duplicates.itertuples():\n",
    "    documents[duplicate.Index] = None\n",
    "\n",
    "# Remove the None elements from the documents list\n",
    "documents = [element for element in documents if element is not None]\n",
    "print(f\"Number of documents after removing duplicates {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the total number of tokens to estimate the cost\n",
    "\n",
    "# Load the OpenAI embeddings model in tiktoken\n",
    "encoding = tiktoken.encoding_for_model(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Calculate the total number of tokens in the documents\n",
    "tokens = 0\n",
    "for doc in documents:\n",
    "    tokens += len(encoding.encode(doc.page_content))\n",
    "print(f\"Total number of tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Save the documents to the Chroma database\n",
    "\n",
    "# # Save the documents to the Chroma database\n",
    "# # The embeddings are calculated using the OpenAI embeddings model\n",
    "# # The database is saved in the chroma_db folder with the collection name \"lyrics\"\n",
    "# Chroma.from_documents(\n",
    "#     documents=documents,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"./chroma_db\",\n",
    "#     collection_name=\"lyrics\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
